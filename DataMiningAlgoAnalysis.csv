Name,Description,Pros,Cons,Tips
DecisionTreeClassifier,"Decision Tree Classifier is a simple and widely used classification technique. It applies a straitforward idea to solve the classification problem. Decision Tree Classifier poses a series of carefully crafted questions about the attributes of the test record. Each time time it receive an answer, a follow-up question is asked until a conclusion about the calss label of the record is reached.","Compared to other algorithms decision trees requires less effort for data preparation during pre-processing. A decision tree does not require
normalization of data. A decision tree does not require scaling of data
as well.Missing values in the data also does NOT affect the process of building decision tree to any considerable extent.A Decision trees model
is very intuitive and easy to explain to technical teams as well as stakeholders.",A small change in the data can cause a large change in the structure of the decision tree causing instability. For a Decision tree sometimes calculation can go far more complex compared to other algorithms. Decision tree often involves higher time to train the model.Decision tree training is relatively expensive as complexity and time taken is more.Decision Tree algorithm is inadequate for applying regression and predicting continuous values.,Decision trees can become much more powerful when used as ensembles. Ensembles are clever ways of combining decision trees to create a more powerful model. These ensembles create state of the art machine learning algorithms that can outperform neural networks in some cases. The two most popular ensemble techniques are random forests and gradient boosting.
ExtraTreeClassifier,Extra Tree Classifier a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.,"builds multiple trees with bootstrap = False by default, which means it samples without replacement",nodes are split based on random splits among a random subset of the features selected at every node,"The Extra Trees classifier performs similar to the Random Forest. However, there are performance differences which is Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance."
AdaBoostClassifier,"An AdaBoost regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the  regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.","    Very good use of weak classifiers for cascading;
    Different classification algorithms can be used as weak classifiers;
    AdaBoost has a high degree of precision;
    Relative to the bagging algorithm andRandom ForestAlgorithm, AdaBoost fully considers the weight of each classifier;","Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. Noisy data, specifically noise in the output variable can be problematic","some heuristics for best preparing your data for AdaBoost:

    1. Quality Data: Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality.
    2. Outliers: Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset.
    3. Noisy Data: Noisy data, specifically noise in the output variable can be problematic. If possible, attempt to isolate and clean these from your training dataset.
"
BaggingClassifier,"A Bagging Classifier is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.",Bagging takes the advantage of ensemble learning wherein multiple weak learner outperform a single strong learner. It helps reduce variance and thus helps us avoid overfitting.,"There is loss of interpretability of the model. There can possibly be a problem of high bias if not modeled properly. Another important disadvantage is that while bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.","Boosting and bagging are two ensemble methods capable of squeezing additional predictive accuracy out of classification algorithms. When using either method, careful tuning of the hyper-parameters should be done to find an optimal balance of model flexibility, efficiency & predictive improvement."
ExtraTreeRegressor,An extra-trees regressor is a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.,"builds multiple trees with bootstrap = False by default, which means it samples without replacement",nodes are split based on random splits among a random subset of the features selected at every node,"The Extra Trees regressor performs similar to the Random Forest. However, there are performance differences which is Decision Trees show high variance, Random Forests show medium variance, and Extra Trees show low variance."
DecisionTreeRegressor,"Decision Tree Regressor is a simple and widely used regression technique. It applies a straitforward idea to solve the rehression problem. Decision Tree Regressor poses a series of carefully crafted questions about the attributes of the test record. Each time time it receive an answer, a follow-up question is asked until a conclusion about the calss label of the record is reached.","Compared to other algorithms decision trees requires less effort for data preparation during pre-processing. A decision tree does not require
normalization of data. A decision tree does not require scaling of data
as well.Missing values in the data also does NOT affect the process of building decision tree to any considerable extent.A Decision trees model
is very intuitive and easy to explain to technical teams as well as stakeholders.",A small change in the data can cause a large change in the structure of the decision tree causing instability. For a Decision tree sometimes calculation can go far more complex compared to other algorithms. Decision tree often involves higher time to train the model.Decision tree training is relatively expensive as complexity and time taken is more.Decision Tree algorithm is inadequate for applying regression and predicting continuous values.,Decision trees can become much more powerful when used as ensembles. Ensembles are clever ways of combining decision trees to create a more powerful model. These ensembles create state of the art machine learning algorithms that can outperform neural networks in some cases. The two most popular ensemble techniques are random forests and gradient boosting.
AdaBoostRegressor,"An AdaBoost] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.",Handles lots of irrelevant features well (separates signal from noise),Does not Performs well with small number of observations,"some heuristics for best preparing your data for AdaBoost:

    1. Quality Data: Because the ensemble method continues to attempt to correct misclassifications in the training data, you need to be careful that the training data is of a high-quality.
    2. Outliers: Outliers will force the ensemble down the rabbit hole of working hard to correct for cases that are unrealistic. These could be removed from the training dataset.
    3. Noisy Data: Noisy data, specifically noise in the output variable can be problematic. If possible, attempt to isolate and clean these from your training dataset.
"
BaggingRegressor,"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.",Bagging takes the advantage of ensemble learning wherein multiple weak learner outperform a single strong learner. It helps reduce variance and thus helps us avoid overfitting.,"There is loss of interpretability of the model. There can possibly be a problem of high bias if not modeled properly. Another important disadvantage is that while bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.","Boosting and bagging are two ensemble methods capable of squeezing additional predictive accuracy out of classification algorithms. When using either method, careful tuning of the hyper-parameters should be done to find an optimal balance of model flexibility, efficiency & predictive improvement."
GradientBoostingRegressor,Gradient Boosting builds an additive model in a forward stage-wise fashion; It allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.,Often provides predictive accuracy that cannot be beat. Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible. No data pre-processing required - often works great with categorical and numerical values as is.,Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.,It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.
Ridge,"Ridge solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
","Least squares regression doesn’t differentiate “important” from “less-important” predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. Ridge regression avoids these problems.
Ridge regression works in part because it doesn’t require unbiased estimators; while least squares produce unbiased estimates; its variances can be so large that they may be wholly inaccurate.
Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.
One important advantage of the ridge regression is that it still performs well, compared to the ordinary least square method in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n).
The ridge estimator is especially good at improving the least-squares estimate when multicollinearity is present.","Firstly ridge regression includes all the predictors in the final model, unlike the stepwise regression methods which will generally select models that involve a reduced set of variables.
A ridge model does not perform feature selection. If a greater interpretation is necessary where we need to reduce the signal in our data to a smaller subset then a lasso model may be preferable.
Ridge regression shrinks the coefficients towards zero, but it will not set an",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
Lasso,Lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. ,LASSO is a penalized regression method to improve OLS and Ridge regression.LASSO does shrinkage and variable selection simultaneously for better prediction and modelinterpretation,LASSO selects at mostnvariables before it saturates.LASSO can not dogroup selection.,Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
LassoLARs,"LassoLars is a lasso model implemented using the LARS algorithm, and unlike the implementation based on coordinate descent, this yields the exact solution, which is piecewise linear as a function of the norm of its coefficients.",", LARS is a particular method to solve the Lasso problem, i.e. the l1-regularized least squares problem. Its success stems from the fact that it requires an asymptotic effort comparable to standard least-squares regression, and thus a highly superior performance than required by the solution of a quadratic programming problem.","When used in stage-wise mode, the LARS algorithm is a greedy method that does not yield a provably consistent estimator",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
SVM-RBF,Support Vector regression is a type of Support vector machine that supports linear and non-linear regression. In SVR we try to fit the error within a certain threshold.,"They’re accurate in high dimensional spaces and, they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient.","The algorithm is prone for over-fitting, if the number of features is much greater than the number of samples.Also, SVMs do not directly provide probability estimates, which are desirable in most classification problems.And
 finally, SVMs are not very efficient computationally, if your dataset 
is very big, such as when you have more than one thousand rows.",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
SVM-POLY,Support Vector regression is a type of Support vector machine that supports linear and non-linear regression. In SVR we try to fit the error within a certain threshold.,"They’re accurate in high dimensional spaces and, they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient.","The algorithm is prone for over-fitting, if the number of features is much greater than the number of samples.Also, SVMs do not directly provide probability estimates, which are desirable in most classification problems.And
 finally, SVMs are not very efficient computationally, if your dataset 
is very big, such as when you have more than one thousand rows.",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
SVM-LINEAR,Support Vector regression is a type of Support vector machine that supports linear and non-linear regression. In SVR we try to fit the error within a certain threshold.,"They’re accurate in high dimensional spaces and, they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient.","The algorithm is prone for over-fitting, if the number of features is much greater than the number of samples.Also, SVMs do not directly provide probability estimates, which are desirable in most classification problems.And
 finally, SVMs are not very efficient computationally, if your dataset 
is very big, such as when you have more than one thousand rows.",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
GradientBoostingClassifier,Gradient Boosting builds an additive model in a forward stage-wise fashion; It allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.,Often provides predictive accuracy that cannot be beat. Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible. No data pre-processing required - often works great with categorical and numerical values as is.,Gradient boosting is a greedy algorithm and can overfit a training dataset quickly.,It can benefit from regularization methods that penalize various parts of the algorithm and generally improve the performance of the algorithm by reducing overfitting.
RidgeClassifier,"Ridge solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
","Least squares regression doesn’t differentiate “important” from “less-important” predictors in a model, so it includes all of them. This leads to overfitting a model and failure to find unique solutions. Ridge regression avoids these problems.
Ridge regression works in part because it doesn’t require unbiased estimators; while least squares produce unbiased estimates; its variances can be so large that they may be wholly inaccurate.
Ridge regression adds just enough bias to make the estimates reasonably reliable approximations to true population values.
One important advantage of the ridge regression is that it still performs well, compared to the ordinary least square method in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n).
The ridge estimator is especially good at improving the least-squares estimate when multicollinearity is present.","Firstly ridge regression includes all the predictors in the final model, unlike the stepwise regression methods which will generally select models that involve a reduced set of variables.
A ridge model does not perform feature selection. If a greater interpretation is necessary where we need to reduce the signal in our data to a smaller subset then a lasso model may be preferable.
Ridge regression shrinks the coefficients towards zero, but it will not set an",Conduct research to understand the study area before starting.Use large quantities of reliable data and a few independent variables with well established relationships.Use sound reasoning to determine which variables to include in the regression model.
SGDClassifier,"SGDClassifier is a Linear classifiers (SVM, logistic regression, a.o.) with SGD training.
This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).","They’re accurate in high dimensional spaces and, they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient.",SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations. SGD is sensitive to feature scaling.,This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning
Linear Regression,"They’re accurate in high dimensional spaces and, they use a subset of training points in the decision function (called support vectors), so it’s also memory efficient.",Advantages include how simple it is and ease with implementation ,disadvantages include how is' lack of practicality and how most problems in our real world aren't “linear”.,"The best fit line is the one that minimises sum of squared differences between actual and estimated results. Taking average of minimum sum of squared difference is known as Mean Squared Error (MSE). Smaller the value, better the regression model."